{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"useful_cells.ipynb","provenance":[{"file_id":"1nJWA9rPkPrjjjtwN_vKUSQoomdfWLAFV","timestamp":1639170353786}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n0-bUmXNDq9W"},"source":["#Potentially useful code snippets for your final projects\n","these cells were all part of draft versions of HW1, but we decided to exclude them from the final version. most of them require the dependencies loaded in the HW1 notebook"]},{"cell_type":"markdown","metadata":{"id":"HO0oHgnQt16k"},"source":["## Text generation\n","No assignments in this section, but we will see how to use a pretrained generative language model to perform the task of *open-ended text generation* (e.g., conditional story generation and contextual text continuation). Given an input text passage as context, the task is to generate text that forms a coherent continuation from the given context. There has been an increasing interest in open-ended text generation due to significant advances in pretrained generative language models, e.g., `GPT-2` [(Radford et al., 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), `XLNet` [(Yang et al., 2020)](https://arxiv.org/pdf/1906.08237.pdf), and `CTRL` [(Keskar et al., 2020)](https://einstein.ai/presentations/ctrl.pdf). The ability of those models to generate coherent text is very impressive, e.g., [GPT2 on unicorns](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [Fill-in-the-blank text generation with T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html). Below you will see how you can implement open-ended text generation with very little effort.\n","\n","First, run the cell below to download a pretrained generative model, i.e., `XLNet` from S3."]},{"cell_type":"code","metadata":{"id":"ih5YALFvFlg9"},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","model_name_or_path = \"xlnet-base-cased\"\n","cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n","model = AutoModelWithLMHead.from_pretrained(model_name_or_path, cache_dir=cache_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJ92stqPo58U"},"source":["Let's use the model to generate text for a given context passage. The following cell generates a different text each time you run, so try it several times to see how it goes. "]},{"cell_type":"code","metadata":{"id":"YOxNmvBFpKsC"},"source":["# Padding text helps XLNet with short contexts - proposed by Aman Rusia in\n","# https://github.com/rusiaaman/XLNet-gen#methodology\n","PADDING_TEXT = \"\"\"Natural Language Processing (NLP) is the engineering art and \n","science of how to teach computers to understand human language. NLP is a type \n","of artificial intelligence technology, and it's now ubiquitous -- NLP lets us \n","talk to our phones, use the web to answer questions, map out discussions in \n","books and social media, and even translate between human languages. Since \n","language is rich, ambiguous, and very difficult for computers to understand, \n","these systems can sometimes seem like magic -- but these are engineering \n","problems we can tackle with data, math, and insights from linguistics.<eod> \n","</s> <eos>\"\"\" # can be a random text\n","\n","context = \"I am taking a course on natural language processing. So far it has been\"\n","inputs = tokenizer.encode(PADDING_TEXT + context, add_special_tokens=False, \n","                          return_tensors=\"pt\")\n","\n","context_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, \n","                                      clean_up_tokenization_spaces=True))\n","outputs = model.generate(inputs, max_length=250, do_sample=True, \n","                         top_p=0.95, top_k=60)\n","generated_text = context + tokenizer.decode(outputs[0])[context_length:]\n","print(generated_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMwX80FMrBbS"},"source":["## Machine translation (optional, 0 points)\n","\n","Again, no assignments in this section. We will try `T5` [(Raffel et al., 2020)](https://arxiv.org/pdf/1910.10683.pdf), an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. The model works well on a variety of downstream NLP tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g.: for translation: `translate English to German: ...`, for summarization: `summarize: ...`.\n","\n","Run the following cell to download the pretrained model from S3."]},{"cell_type":"code","metadata":{"id":"-5yrzrwtzUm9"},"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","model_name_or_path = \"t5-base\"\n","cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n","model = AutoModelWithLMHead.from_pretrained(model_name_or_path, cache_dir=cache_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IgXlpF40qjQ"},"source":["Let's try a translation task: `translate English to German`."]},{"cell_type":"code","metadata":{"id":"BXYHAT6z0p2O"},"source":["source = \"English\"\n","target = \"German\" #  German, French, or Romanian\n","sentence = \"This course will broadly focus on deep learning methods for \\\n","natural language processing.\"\n","inputs = tokenizer.encode(f\"translate {source} to {target}: {sentence}\", \n","                          return_tensors=\"pt\")\n","outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)\n","print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(outputs.squeeze())))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6OsOgr6zfC6"},"source":["# Fine-tuning BERT\n","If you would like to see how fine-tuning is performed, you can use the cell below to fine-tune `BERT` on a small subset (1K examples) of the dataset which we call `smallSST`, located at `data/smallSST` (please uncomment the code before you run, it would take around 1-2 minutes)."]},{"cell_type":"code","metadata":{"id":"c3gs8kWmzfXj"},"source":["start_time = timeit.default_timer()\n","task_name = \"SST\"\n","data_dir = f\"./data/small{task_name}\"\n","model_name_or_path = \"bert-base-cased\"\n","model_cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","data_cache_dir = f\"./data_cache/finetuning/small{task_name}\"\n","output_dir = f\"./output/finetuning/bert-finetuned-small{task_name}\"\n","\n","do_target_task_finetuning(\n","    model_name_or_path=model_name_or_path,\n","    task_name=f\"{task_name}-2\",\n","    task_type=\"text_classification\",\n","    do_train=True,\n","    # do_eval=True, # you can do evaluation while training \n","    do_lower_case=True,\n","    data_dir=data_dir,\n","    max_seq_length=128,\n","    per_device_train_batch_size=32,\n","    learning_rate=2e-5,\n","    num_train_epochs=3.0,\n","    model_cache_dir=model_cache_dir,\n","    data_cache_dir=data_cache_dir,\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n",")\n","elapsed_time = timeit.default_timer() - start_time\n","print(f\"Time elapsed: {elapsed_time} seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wt4_JfmX7w5d"},"source":["## Fine-tuning BERT for question answering\n","In this section, we will use `BERT` for a question answering task, i.e., `SQuAD` [(Rajpurkar et al., 2016)](https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf) whose dataset was built from Wikipedia. If you would like to see how fine-tuning is perfomed, uncomment the code in the cell below and run it to fine-tune `BERT` on a small subset (1K examples) of the dataset which we call `smallSQuaAD`, located at `data/smallSQuAD` (it would take around 4 minutes)."]},{"cell_type":"code","metadata":{"id":"DHePUbo38Sm7"},"source":["start_time = timeit.default_timer()\n","task_name = \"SQuAD\"\n","data_dir = f\"./data/small{task_name}\"\n","model_name_or_path = \"bert-base-cased\"\n","model_cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","data_cache_dir = f\"./data_cache/finetuning/small{task_name}\"\n","output_dir = f\"./output/finetuning/small{task_name}\"\n","\n","do_target_task_finetuning(\n","    model_type=\"bert\",\n","    model_name_or_path=\"bert-base-cased\",\n","    task_type=\"question_answering\",\n","    do_train=True,\n","    do_eval=False,\n","    do_lower_case=True,\n","    data_dir=data_dir,\n","    train_file=\"train-v1.1.json\",\n","    dev_file=\"dev-v1.1.json\",\n","    per_gpu_train_batch_size=12,\n","    per_gpu_eval_batch_size=8,\n","    learning_rate=3e-5,\n","    num_train_epochs=3.0,\n","    max_seq_length=384,\n","    doc_stride=128,\n","    model_cache_dir=model_cache_dir,\n","    data_cache_dir=data_cache_dir,\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n",")\n","elapsed_time = timeit.default_timer() - start_time\n","print(f\"Time elapsed: {elapsed_time} seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2h-93s617-c"},"source":["You can uncomment the cell below to evaluate the trained model on the `SQuAD`'s dev set. It would take around 6-8 minutes. You should see an F1 score of 87.8% if you run it."]},{"cell_type":"code","metadata":{"id":"RisvV2MS4Svi"},"source":["start_time = timeit.default_timer()\n","task_name = \"SQuAD\"\n","data_dir = f\"./data/small{task_name}\"\n","model_name_or_path = \"bert-base-cased-finetuned-squad\"\n","model_cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","data_cache_dir = f\"./data_cache/finetuning/small{task_name}\"\n","output_dir = f\"./output/finetuning/small{task_name}\"\n","\n","do_target_task_finetuning(\n","    model_type=\"bert\",\n","    model_name_or_path=model_cache_dir,\n","    task_type=\"question_answering\",\n","    do_train=False,\n","    do_eval=True,\n","    do_lower_case=True,\n","    data_dir=data_dir,\n","    train_file=\"train-v1.1.json\",\n","    dev_file=\"dev-v1.1.json\",\n","    per_gpu_eval_batch_size=8,\n","    max_seq_length=384,\n","    doc_stride=128,\n","    model_cache_dir=model_cache_dir,\n","    data_cache_dir=data_cache_dir,\n","    output_dir=output_dir\n",")\n","elapsed_time = timeit.default_timer() - start_time\n","print(f\"Time elapsed: {elapsed_time} seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9LjSL0EXFIGM"},"source":["## Fine-tuning BERT for sequence labeling\n","Next, we will use `BERT` for a named entity recognition (`NER`) task, i.e., `CoNLL-2003` [(Tjong Kim Sang and De Meulder, 2003)](https://www.aclweb.org/anthology/W03-0419.pdf), which was built from news data. For each word in a given input text, the task is to identify if the word is inside a named entity or not by assigning it to one of the following tags:\n","\n","*   `O`: Outside of a named entity\n","\n","*   `B-MISC`: Beginning of a miscellaneous entity right after another miscellaneous entity\n","\n","*   `I-MISC`: Miscellaneous entity\n","\n","*   `B-PER`: Beginning of a person's name right after another person's name\n","\n","*   `I-PER`: Person's name\n","\n","*   `B-ORG`: Beginning of an organisation right after another organisation\n","\n","*   `I-ORG`: Organisation\n","\n","*   `B-LOC`: Beginning of a location right after another location\n","\n","*   `I-LOC`: Location.\n","\n","You can uncomment and run the cell below to see how fine-tunign is performed on a small subset (1K examples) of the dataset which we call `smallNER`,located at `data/smallNER`(it would take around 2-3 minutes)."]},{"cell_type":"code","metadata":{"id":"_ZXVIaeaGNqf"},"source":["start_time = timeit.default_timer()\n","task_name = \"NER\"\n","data_dir = f\"./data/small{task_name}\"\n","label_file = f\"{data_dir}/labels.txt\"\n","model_name_or_path = \"bert-base-cased\"\n","model_cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","data_cache_dir = f\"./data_cache/finetuning/small{task_name}\"\n","output_dir = f\"./output/finetuning/small{task_name}\"\n","\n","do_target_task_finetuning(\n","    model_type=\"bert\",\n","    model_name_or_path=\"bert-base-cased\",\n","    task_type=\"sequence_labeling\",\n","    do_train=True,\n","    do_eval=False,\n","    do_lower_case=False,\n","    data_dir=data_dir,\n","    labels=label_file,\n","    max_seq_length=128,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=6.0,\n","    model_cache_dir=model_cache_dir,\n","    data_cache_dir=data_cache_dir,\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n",")\n","elapsed_time = timeit.default_timer() - start_time\n","print(f\"Time elapsed: {elapsed_time} seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrqhfi4yG5Es"},"source":["Again, we will provide you with a model trained on the full `CoNLL-2003` dataset of 14K examples. Run the following cell to download the trained model."]},{"cell_type":"code","metadata":{"id":"3dB2RPqQHbEH"},"source":["data_file = drive.CreateFile({'id': '1JJIGk6PS9U7C0zoTk121ZfYXhfMxZWTh'})\n","data_file.GetContentFile('bert-base-cased-finetuned-ner.zip')\n","\n","# Extract the data from the zipfile and put it into pretrained_models_dir\n","with zipfile.ZipFile('bert-base-cased-finetuned-ner.zip', 'r') as zip_file:\n","    zip_file.extractall(pretrained_models_dir)\n","os.remove('bert-base-cased-finetuned-ner.zip')\n","print(\"bert-base-cased-finetuned-ner downloaded!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EDtlWbf_VVDm"},"source":["\n","Now, let's use the trained model to make a few predictions. Your task is to complete the code below to show the predicted named entity tag for each token of a given input sentence."]},{"cell_type":"code","metadata":{"id":"PANcYWSlRIe6"},"source":["sequence = \"The University of Massachusetts Amherst is a public research and land-grant university in Amherst, \" \\\n","            \"Massachusetts. It is the flagship campus of the University of Massachusetts system.\"\n","\n","# YOUR CODE HERE!\n","\n","##### SOLUTION #####\n","task_name = \"NER\"\n","data_dir = f\"./data/small{task_name}\"\n","label_file = f\"{data_dir}/labels.txt\"\n","model_name_or_path = \"bert-base-cased-finetuned-ner\"\n","pretrained_weights = os.path.join(pretrained_models_dir, model_name_or_path)\n","task_type = \"sequence_labeling\"\n","model = AUTO_MODEL[task_type].from_pretrained(pretrained_weights)\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n","\n","with open(label_file, \"r\") as f:\n","  label_list = f.read().splitlines()\n","\n","# Get the tokens with the special tokens\n","tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n","inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n","\n","outputs = model(inputs)[0]\n","predictions = torch.argmax(outputs, dim=2)\n","\n","for token, prediction in zip(tokens, predictions[0].tolist()):\n","  print(f\"{token}: {label_list[prediction]}\")\n","##### END SOLUTION #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHeVzpXfr71c"},"source":["## Further pretraining on the target training data\n","Another approach to improve the perfomance of `BERT` on a target task is to further pretrain it with the masked language modeling objective on the target task's unlabeled data before fine-tuning with the target task's supervised data. The intuition is that the target task's data likely comes from a different distribution than the general-domain data used for pretraining `BERT`, no matter how diverse the pretraining data is. Previous work demonstrated that this can lead to significant performance gains [(Howard and Ruder, 2018](https://arxiv.org/pdf/1801.06146.pdf), [Gururangan et al., 2020)](https://arxiv.org/pdf/2004.10964.pdf).\n","\n","In the following cell, we provide you with a function for fine-tuning `BERT` with the language modeling objective on the target task's unlabeled data. No need to edit any code, just run the cell."]},{"cell_type":"code","metadata":{"id":"wjJ3ZZ9swmks"},"source":["def do_target_task_LM_finetuning(model_name_or_path, output_dir, **kwargs):\n","    r\"\"\" Fine-tuning BERT on a downstream target task.\n","    Params:\n","        **model_name_or_path**: either:\n","            - a string with the `shortcut name` of a pre-trained model configuration to load from cache\n","                or download and cache if not already stored in cache (e.g. 'bert-base-uncased').\n","            - a path to a `directory` containing a configuration file saved\n","                using the `save_pretrained(save_directory)` method.\n","            - a path or url to a saved configuration `file`.\n","        **output_dir**: string:\n","            The output directory where the model predictions and checkpoints will be written.\n","        **kwargs**: (`optional`) dict:\n","            Dictionary of key/value pairs with which to update the configuration object after loading.\n","            - The values in kwargs of any keys which are configuration attributes will be used\n","            to override the loaded values.\n","    \"\"\"\n","    model_args = ModelArguments(model_name_or_path=model_name_or_path)\n","    data_args = LMDataTrainingArguments()\n","    training_args = TrainingArguments(output_dir=output_dir)\n","\n","    # override the loaded configs\n","    configs = (model_args, data_args, training_args)\n","    for config in configs:\n","        for key, value in kwargs.items():\n","            if hasattr(config, key):\n","                setattr(config, key, value)\n","\n","    if data_args.eval_data_file is None and training_args.do_eval:\n","        raise ValueError(\n","            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n","            \"or remove the --do_eval argument.\"\n","        )\n","\n","    if (\n","        os.path.exists(training_args.output_dir)\n","        and os.listdir(training_args.output_dir)\n","        and training_args.do_train\n","        and not training_args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","            f\"Use --overwrite_output_dir to overcome.\"\n","        )\n","\n","    for p in [model_args.model_cache_dir, training_args.output_dir]:\n","        if not os.path.exists(p):\n","            os.makedirs(p)\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","\n","    logger.info(\"Process device: %s, n_gpu: %s\", training_args.device, training_args.n_gpu)\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed\n","    set_seed(training_args.seed)\n","\n","    # Load pretrained model and tokenizer\n","\n","    config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.model_cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.model_cache_dir)\n","    model = AutoModelWithLMHead.from_pretrained(model_args.model_name_or_path, from_tf=False, config=config,\n","                                                cache_dir=model_args.model_cache_dir)\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    assert data_args.mlm, \"BERT must be run using the --mlm flag (masked language modeling).\"\n","\n","    if data_args.block_size <= 0:\n","        data_args.block_size = tokenizer.max_len\n","        # Our input block size will be the max possible for the model\n","    else:\n","        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n","\n","    # Get datasets\n","\n","    train_dataset = (get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.data_cache_dir)\n","                     if training_args.do_train else None)\n","    eval_dataset = (get_dataset(data_args, tokenizer=tokenizer, evaluate=True, cache_dir=model_args.data_cache_dir)\n","                    if training_args.do_eval else None)\n","\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n","    )\n","\n","    # Initialize our Trainer\n","    trainer_params = {\n","        \"model\": model,\n","        \"args\": training_args,\n","        \"train_dataset\": train_dataset,\n","        \"eval_dataset\": eval_dataset,\n","        \"data_collator\": data_collator,\n","        \"prediction_loss_only\": True,\n","    }\n","    trainer = Trainer(**trainer_params)\n","\n","    # Training\n","    if training_args.do_train:\n","        trainer.train(\n","            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","        )\n","        trainer.save_model()\n","        # For convenience, we also re-save the tokenizer to the same directory\n","        tokenizer.save_pretrained(training_args.output_dir)\n","\n","    # Evaluation\n","    eval_results = {}\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","        eval_output = trainer.evaluate()\n","\n","        perplexity = math.exp(eval_output[\"eval_loss\"])\n","        eval_result = {\"perplexity\": perplexity}\n","\n","        output_eval_file = os.path.join(training_args.output_dir, f\"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            logger.info(\"***** Eval results *****\")\n","            for key, value in eval_result.items():\n","                logger.info(\"  %s = %s\", key, value)\n","                writer.write(\"%s = %s\\n\" % (key, value))\n","\n","        eval_results.update(eval_result)\n","    return eval_results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vj56b0HQ0iZD"},"source":["You can use the cell below to perform target task language model fine-tuning `BERT` on a subset (1000 sentences) of the unlabeled data part of the original `SST` dataset (please uncomment the code before you run, it would take around 1 minute). The data file should be a plain text file, with one sentence per line, see `sents_train.txt` in `data/tinySST`."]},{"cell_type":"code","metadata":{"id":"lVKpO9tG1Jfw"},"source":["start_time = timeit.default_timer()\n","task_name = \"SST\"\n","data_dir = f\"./data/tiny{task_name}\"\n","model_name_or_path = \"bert-base-cased\"\n","model_cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","data_cache_dir = f\"./data_cache/lm-finetuning/tiny{task_name}/\"\n","output_dir = f\"./output/lm-finetuning/tiny{task_name}\"\n","\n","# Target task LM fine-tuning\n","start_time = timeit.default_timer()\n","do_target_task_LM_finetuning(\n","    model_name_or_path=model_name_or_path,\n","    model_type=\"bert\",\n","    do_train=True,\n","    train_data_file=f\"{data_dir}/sents_train.txt\",\n","    line_by_line=True,\n","    do_eval=True,\n","    eval_data_file=f\"{data_dir}/sents_dev.txt\",\n","    model_cache_dir=model_cache_dir,\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n","    mlm=True,\n",")\n","\n","# Evaluate\n","model_dir = f\"./output/lm-finetuning/tiny{task_name}\"\n","do_target_task_LM_finetuning(\n","    model_name_or_path=model_dir,\n","    model_type=\"bert\",\n","    do_train=False,\n","    line_by_line=True,\n","    do_eval=True,\n","    eval_data_file=f\"{data_dir}/sents_dev.txt\",\n","    model_cache_dir=model_cache_dir,\n","    output_dir=output_dir,\n","    mlm=True,\n",")\n","elapsed_time = timeit.default_timer() - start_time\n","print(f\"Time elapsed: {elapsed_time} seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgH7D9mxAuyN"},"source":["Since fine-tuning `BERT` with the language modeling objective on the whole unlabeled data of the original `SST` dataset (67K examples) would take a while, we will provide you with a trained model to save your time. Run the following cell to download the model."]},{"cell_type":"code","metadata":{"id":"VmRwZd4PJni6"},"source":["data_file = drive.CreateFile({'id': '1PwiB5CsziaqfSq6WjxVnqGt7AWU5r_WK'})\n","data_file.GetContentFile('bert-base-cased-lm-finetuned-sst.zip')\n","\n","# Extract the data from the zipfile and put it into the data directory\n","with zipfile.ZipFile('bert-base-cased-lm-finetuned-sst.zip', 'r') as zip_file:\n","    zip_file.extractall(pretrained_models_dir)\n","os.remove('bert-base-cased-lm-finetuned-sst.zip')\n","print(\"bert-base-cased-lm-finetuned-sst downloaded!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wxddQ0xLbMhg"},"source":["The following cell fine-tunes the `BERT` model `bert-base-cased-lm-finetuned-sst` with the target task `tinySST`'s supervised data and then evaluate the resulting model on `tinySST`'s dev set."]},{"cell_type":"code","metadata":{"id":"mifHOgNxW_um"},"source":["start_time = timeit.default_timer()\n","task_name = \"SST\"\n","data_dir = f\"./data/tiny{task_name}\"\n","model_name_or_path = \"bert-base-cased-lm-finetuned-sst\"\n","model_cache_dir = os.path.join(pretrained_models_dir, model_name_or_path)\n","data_cache_dir = f\"./data_cache/lm-finetuning/tiny{task_name}/\"\n","output_dir = model_cache_dir\n","\n","mean = None\n","std = None\n","\n","# Fine-tune BERT for 20 epochs using 4 random seeds\n","for seed in [1234, 2341, 3412, 4123]:\n","  output_dir = f\"./output/tiny{task_name}-{seed}\"\n","  # YOUR CODE HERE!\n","\n","  ##### SOLUTION #####\n","  do_target_task_finetuning(\n","      seed=seed,\n","      model_name_or_path=model_cache_dir,\n","      task_name=f\"{task_name}-2\",\n","      task_type=\"text_classification\",\n","      do_train=True,\n","      do_eval=False, \n","      do_lower_case=True,\n","      data_dir=data_dir,\n","      max_seq_length=128,\n","      per_device_train_batch_size=32,\n","      learning_rate=2e-5,\n","      num_train_epochs=20.0,\n","      model_cache_dir=model_cache_dir,\n","      data_cache_dir=data_cache_dir,\n","      output_dir=output_dir,\n","      overwrite_output_dir=True,\n","  )\n","##### END SOLUTION #####\n","\n","# Evaluate BERT on the dev set\n","results = []\n","for seed in [1234, 2341, 3412, 4123]:\n","  # YOUR CODE HERE!\n","  \n","  ##### SOLUTION #####\n","  model_dir = f\"./output/tiny{task_name}-{seed}\"\n","  result = do_target_task_finetuning(\n","      seed=seed,\n","      model_name_or_path=model_dir,\n","      task_name=f\"{task_name}-2\",\n","      task_type=\"text_classification\",\n","      do_train=False,\n","      do_eval=True, \n","      do_lower_case=True,\n","      data_dir=data_dir,\n","      max_seq_length=128,\n","      model_cache_dir=model_cache_dir,\n","      data_cache_dir=data_cache_dir,\n","      output_dir=model_dir,\n","  )\n","  results.append(result[\"eval_acc\"])\n","\n","results = np.array(results)\n","mean = np.mean(results)\n","std = np.std(results)\n","##### END SOLUTION #####\n","\n","print(\"===== Target task language model fine-tuning =====\")\n","print(f\"Performance when fine-tuning BERT for 20 epochs: {mean} +/- {std}\")\n","elapsed_time = timeit.default_timer() - start_time\n","print(f\"Time elapsed: {elapsed_time} seconds\")"],"execution_count":null,"outputs":[]}]}